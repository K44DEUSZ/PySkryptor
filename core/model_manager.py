# core/model_manager.py
# ≈Åadowanie lokalnego modelu ASR (np. Whisper-turbo) i budowa pipeline z w≈Ça≈õciwym device/dtype.

from pathlib import Path
from typing import Optional, Callable

import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

from core.config import Config


class ModelManager:
    """
    Odpowiada za:
    - za≈Çadowanie modelu z lokalnego katalogu,
    - przeniesienie na odpowiednie urzƒÖdzenie (GPU/CPU),
    - zbudowanie pipeline Transformers ASR.
    """

    def __init__(self, model_dir: Optional[Path] = None) -> None:
        self.model_dir = Path(model_dir) if model_dir else Config.WHISPER_TURBO_DIR
        self._model: Optional[torch.nn.Module] = None
        self._processor = None
        self._pipe = None

    def load(self, log: Callable[[str], None] = print) -> None:
        """
        ≈Åaduje model i procesor lokalnie, tworzy pipeline.
        Loguje tryb (GPU/CPU), nazwƒô urzƒÖdzenia, dtype i TF32.
        """
        # Log o trybie
        mode = "GPU" if Config.DEVICE.type == "cuda" else "CPU"
        dtype_name = str(Config.DTYPE).split(".")[-1]
        tf32 = "ON" if Config.TF32_ENABLED else "OFF"
        dev_name = Config.DEVICE_FRIENDLY_NAME

        log(f"üß† Tryb: {mode}{f' ({dev_name})' if mode == 'GPU' else ''}, dtype={dtype_name}, TF32={tf32}")

        # Pr√≥ba za≈Çadowania w preferowanej konfiguracji
        try:
            self._model = AutoModelForSpeechSeq2Seq.from_pretrained(
                str(self.model_dir),
                torch_dtype=Config.DTYPE,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                local_files_only=True,
            )
            self._processor = AutoProcessor.from_pretrained(
                str(self.model_dir),
                local_files_only=True,
                use_safetensors=True,
            )
        except Exception as e:
            # Fallback do CPU (lokalny)
            log(f"‚ùó Nie uda≈Ço siƒô za≈Çadowaƒá modelu lokalnie z ustawieniami GPU/DTYPE ({e}).")
            log("üîÅ Prze≈ÇƒÖczam na tryb CPU fallback‚Ä¶")
            self._model = AutoModelForSpeechSeq2Seq.from_pretrained(
                str(self.model_dir),
                torch_dtype=torch.float32,  # CPU
                low_cpu_mem_usage=True,
                use_safetensors=True,
                local_files_only=True,
            )
            self._processor = AutoProcessor.from_pretrained(
                str(self.model_dir),
                local_files_only=True,
                use_safetensors=True,
            )

            # Wersja CPU
            self._model.to(torch.device("cpu"))
            self._model.eval()

            self._pipe = pipeline(
                task="automatic-speech-recognition",
                model=self._model,
                tokenizer=getattr(self._processor, "tokenizer", self._processor),
                feature_extractor=getattr(self._processor, "feature_extractor", self._processor),
                device=-1,           # -1 = CPU
                torch_dtype=None,    # na CPU nie wymuszamy dtype w pipeline
                ignore_warning=True,  # wycisz ostrze≈ºenia dot. chunk√≥w
                generate_kwargs={"task": "transcribe"},
            )
            log("‚úÖ Model i pipeline gotowe.")
            return

        # Przeniesienie modelu na urzƒÖdzenie
        try:
            self._model.to(Config.DEVICE)
        except Exception:
            # awaryjnie CPU
            self._model.to(torch.device("cpu"))
            log("‚ùó Przeniesienie modelu na GPU nie powiod≈Ço siƒô. Pracujƒô na CPU.")
        self._model.eval()

        # Ustal device index dla pipeline: 0 dla GPU, -1 dla CPU
        device_for_pipe = 0 if Config.DEVICE.type == "cuda" else -1
        dtype_for_pipe = None if Config.DEVICE.type != "cuda" else Config.DTYPE

        self._pipe = pipeline(
            task="automatic-speech-recognition",
            model=self._model,
            tokenizer=getattr(self._processor, "tokenizer", self._processor),
            feature_extractor=getattr(self._processor, "feature_extractor", self._processor),
            device=device_for_pipe,
            torch_dtype=dtype_for_pipe,
            ignore_warning=True,              # nie pokazuj ostrze≈ºe≈Ñ o chunk_length_s itp.
            generate_kwargs={"task": "transcribe"},
        )

        log("‚úÖ Model i pipeline gotowe.")

    @property
    def pipe(self):
        return self._pipe
